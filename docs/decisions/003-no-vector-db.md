# ADR 003: Почему без векторной БД (RAG без embeddings)

**Дата:** 2025-10-14
**Статус:** Принято

---

## Контекст

Для проекта NegotiateAI Assistant нужно было решить как организовать доступ к базе знаний (папка `knowledge/` с ~40 документами).

Классический подход для RAG (Retrieval-Augmented Generation):
1. Разбить документы на chunks
2. Создать embeddings для каждого chunk
3. Сохранить в векторную БД (Pinecone, pgvector, Weaviate)
4. При запросе: найти похожие chunks → передать в LLM

Альтернативный подход (наш):
1. Создать индекс документов (`index.md`)
2. Встроить индекс в system prompt
3. Claude сам выбирает какой документ читать
4. Читать документ целиком через `read_document(filepath)`

---

## Решение

Выбрали **подход без векторной БД** - используем индекс в промпте + function calling для чтения документов.

---

## Причины

### 1. Небольшая база знаний (~40 документов)

**Наша ситуация:**
```
knowledge/
├── ~30 DOCX файлов
├── ~10 PDF файлов
└── ~10 папок стран (с 2-5 файлами каждая)

Итого: ~40-50 документов
```

**Размер индекса:**
- index.md: ~2000-3000 токенов
- Список всех документов с кратким описанием
- Легко помещается в system prompt

**Почему векторная БД не нужна:**

Векторный поиск полезен когда:
- ❌ Тысячи документов
- ❌ Нужно искать по семантике в огромной базе
- ❌ Невозможно перечислить все документы

У нас:
- ✅ 40-50 документов - Claude легко навигирует
- ✅ Документы структурированы (по странам, темам)
- ✅ index.md даёт полный обзор

### 2. Claude Sonnet 4.5 имеет большой контекст (200K токенов)

**Контекст Claude:**
- 200,000 токенов input
- Это ~150,000 слов или ~600 страниц текста

**Наш system prompt:**
- index.md: ~3K токенов
- system-prompt.md: ~5K токенов
- **Итого: ~8K токенов** - это 4% от лимита!

**Вывод:** Нет необходимости экономить контекст через векторный поиск.

### 3. Чтение целых документов лучше chunks

**Проблема с chunking:**
```
Документ: "ПОЛЬЗОВАТЕЛЬСКИЙ ПУТЬ ПОСТАВЩИКА.docx"

Разбит на chunks:
- Chunk 1: Регистрация...
- Chunk 2: Создание профиля...
- Chunk 3: Публикация товаров...

Проблема: Потерян контекст между шагами!
```

**Наш подход:**
```typescript
read_document("ПОЛЬЗОВАТЕЛЬСКИЙ ПУТЬ ПОСТАВЩИКА.docx")
// Claude читает весь документ целиком
// Понимает связи между шагами
// Видит полную картину
```

**Преимущества:**
- ✅ Сохранён контекст документа
- ✅ Понятна структура (заголовки, секции)
- ✅ Нет потери информации на границах chunks
- ✅ Claude сам выбирает что важно

### 4. Anthropic API читает DOCX/PDF напрямую

**Нативное чтение:**
```typescript
// Anthropic API умеет читать DOCX/PDF
anthropic.messages.create({
  messages: [{
    role: 'user',
    content: [{
      type: 'document',
      source: { type: 'base64', data: docxBase64 }
    }]
  }]
});
```

**С векторной БД пришлось бы:**
1. Конвертировать DOCX/PDF в текст (потеря форматирования)
2. Разбить на chunks (потеря контекста)
3. Создать embeddings (дополнительный API call)
4. Загрузить в БД (инфраструктура)
5. Поддерживать синхронизацию

**Наш подход:**
1. ✅ Читаем DOCX/PDF напрямую
2. ✅ Сохраняем форматирование
3. ✅ Нет конвертации
4. ✅ Нет инфраструктуры

### 5. Проще и быстрее

**Без векторной БД:**
```
Setup time: 10 минут
- Создать index.md
- Встроить в промпт
- Готово!

Infrastructure: 0
- Нет БД
- Нет embeddings API
- Нет sync jobs
```

**С векторной БД:**
```
Setup time: 2-3 часа
- Настроить Pinecone/pgvector
- Написать chunking logic
- Создать embeddings (OpenAI/Cohere)
- Загрузить в БД
- Настроить sync

Infrastructure:
- Pinecone account ($)
- Embeddings API ($)
- Sync scripts
- Monitoring
```

**Учитывая дедлайн 2-4 часа** - векторная БД не вписывается.

### 6. Стоимость

**Без векторной БД:**
- Anthropic API: ~$3-15 за 1M токенов
- Итого: ~$5-10/месяц для типичного usage

**С векторной БД:**
- Anthropic API: $3-15 за 1M токенов
- Embeddings API (OpenAI): $0.13 за 1M токенов
- Pinecone: $70/месяц (Starter plan)
- Итого: ~$80-100/месяц

---

## Последствия

### Плюсы

- ✅ **Простота** - минимальная инфраструктура
- ✅ **Скорость разработки** - готово за 10 минут
- ✅ **Нет потери контекста** - читаем документы целиком
- ✅ **Нативное чтение DOCX/PDF** - через Anthropic API
- ✅ **Дёшево** - нет доп. сервисов
- ✅ **Легко обновлять** - просто обнови index.md

### Минусы

- ❌ **Не масштабируется** - для 1000+ документов не подойдёт
- ❌ **Медленнее поиск** - Claude должен прочитать документ целиком
- ❌ **Нет семантического поиска** - Claude выбирает документ по названию/описанию, не по содержимому

### Trade-offs

Мы выбрали **простоту и скорость разработки** над **масштабируемостью**.

Для 40-50 документов это оптимальный выбор.

Если база знаний вырастет до 500+ документов - нужно будет пересмотреть.

---

## Альтернативы

### Альтернатива 1: Pinecone + OpenAI Embeddings

**Что это:** Managed векторная БД + API для создания embeddings

**Архитектура:**
```
1. Chunking: Разбить документы на chunks (512 токенов)
2. Embeddings: Создать vectors через OpenAI Embeddings API
3. Upload: Загрузить в Pinecone
4. Query: При запросе искать top-5 похожих chunks
5. Context: Передать chunks в Claude
```

**Плюсы:**
- Масштабируется до миллионов документов
- Семантический поиск (по смыслу, не по ключевым словам)
- Managed service (не нужно настраивать инфраструктуру)

**Минусы:**
- Дорого ($70/месяц + embeddings API)
- Сложнее setup
- Потеря контекста при chunking
- Нужна синхронизация при обновлении документов

**Когда лучше:**
- База знаний > 500 документов
- Нужен семантический поиск
- Есть бюджет

### Альтернатива 2: pgvector (PostgreSQL extension)

**Что это:** Open-source расширение PostgreSQL для векторного поиска

**Плюсы:**
- Бесплатно (self-hosted)
- Полный контроль
- Можно хостить вместе с другими данными

**Минусы:**
- Нужно настраивать и поддерживать PostgreSQL
- Медленнее Pinecone
- Сложнее масштабировать

**Когда лучше:**
- Уже есть PostgreSQL в проекте
- Нужен self-hosted solution
- Средняя база знаний (100-1000 документов)

### Альтернатива 3: Prompt Caching без векторной БД

**Что это:** Наш текущий подход + кэширование промпта

**Улучшение:**
```typescript
// Anthropic Prompt Caching
const response = await anthropic.messages.create({
  system: [
    {
      type: 'text',
      text: systemPrompt + index,
      cache_control: { type: 'ephemeral' } // Кэш на 5 минут
    }
  ],
  messages: [...]
});
```

**Плюсы:**
- Дешевле (90% discount на кэшированные токены)
- Быстрее (не нужно обрабатывать промпт каждый раз)
- Простота (тот же подход + одна строка кода)

**Когда:**
- Много повторяющихся запросов
- System prompt большой
- Нужно оптимизировать cost

**Примечание:** Можно добавить позже без изменения архитектуры.

---

## Ссылки и ресурсы

- [Claude Context Window](https://docs.anthropic.com/claude/docs/models-overview)
- [Document Support in Claude](https://docs.anthropic.com/claude/docs/vision#document-support)
- [Pinecone Pricing](https://www.pinecone.io/pricing/)
- [When to use RAG vs Long Context](https://www.anthropic.com/research/contextual-retrieval)

---

## Примечания

### Когда пересмотреть решение

Добавить векторную БД если:

1. **База знаний > 500 документов**
   - index.md станет слишком большим
   - Claude не сможет эффективно навигировать

2. **Нужен семантический поиск**
   - Запросы вида "найди все документы о [abstract concept]"
   - Не можем описать в index.md

3. **Частые обновления документов**
   - Добавляются новые документы каждый день
   - Тяжело поддерживать index.md вручную

4. **Оптимизация latency критична**
   - Чтение целого документа медленно (3-5 сек)
   - Нужны только релевантные chunks (1-2 сек)

### Hybrid подход

Можно комбинировать:
1. **index.md для навигации** - Claude выбирает документ
2. **Векторный поиск внутри документа** - находит релевантные части
3. **Передача в Claude** - только нужные chunks

Но для текущего размера базы это overkill.

---

## История изменений

- **2025-10-14** - Документ создан (Владимир)
- Решение принято для MVP с учётом размера базы знаний и дедлайна
